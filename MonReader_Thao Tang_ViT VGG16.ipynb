{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23581435",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "- [Verify GPU](#gpu)\n",
    "- [Data Pipeline](#pipe)\n",
    "- [Call VGG16](#vgg)\n",
    "- [Model evaluation](#eva)\n",
    "- [Conclusion](#con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90cc857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python platform: macOS-14.0-arm64-arm-64bit\n",
      "Tensorflow version: 2.14.0\n",
      "\n",
      "python 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:41:52) [Clang 15.0.7 ]\n",
      "pandas 2.1.1\n",
      "scikit-learn 1.3.1\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import platform\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pathlib\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f'Python platform: {platform.platform()}')\n",
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "\n",
    "print()\n",
    "print(f'python {sys.version}')\n",
    "print(f'pandas {pd.__version__}')\n",
    "print(f'scikit-learn {sk.__version__}')\n",
    "gpu=len(tf.config.list_physical_devices('GPU'))>0\n",
    "print('GPU is', 'available' if gpu else 'not avail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ccffa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    \n",
    "    '''Random seeds for reproducability'''\n",
    "    \n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f207a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c79800",
   "metadata": {},
   "source": [
    "# Verify GPU <a class=\"anchor\" id=\"gpu\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d6000ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "GPU name: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 20:28:43.609250: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-31 20:28:43.609272: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# GPU of my machine that is visible to TensorFlow\n",
    "print('GPU name:',tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b998d4",
   "metadata": {},
   "source": [
    "# Data Pipeline <a class=\"anchor\" id=\"pipe\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "383760a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2392 images in training set\n",
      "(1162 flipped images and 1230 non-flipped images)\n",
      "\n",
      "597 images in testing set\n",
      "(290 flipped images and 307 non-flipped images)\n"
     ]
    }
   ],
   "source": [
    "training_notflip = os.listdir('../APZIVA/images/training/notflip')\n",
    "training_flip = os.listdir('../APZIVA/images/training/flip')\n",
    "testing_notflip = os.listdir('../APZIVA/images/testing/notflip')\n",
    "testing_flip = os.listdir('../APZIVA/images/testing/flip')\n",
    "\n",
    "print(len(training_flip) + len(training_notflip), 'images in training set')\n",
    "print(f'({len(training_flip)} flipped images and {len(training_notflip)} non-flipped images)')\n",
    "print()\n",
    "print(len(testing_flip)+len(testing_notflip),'images in testing set')\n",
    "print(f'({len(testing_flip)} flipped images and {len(testing_notflip)} non-flipped images)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51824617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Found 1914 images belonging to 2 classes.\n",
      "\n",
      "Validation set:\n",
      "Found 478 images belonging to 2 classes.\n",
      "\n",
      "Test set:\n",
      "Found 597 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Let's resize them to size (224,224)\n",
    "image_height=224\n",
    "image_width=224\n",
    "batch_size=16\n",
    "\n",
    "train = ImageDataGenerator(rescale=1/255.0,\n",
    "                            rotation_range=10,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            validation_split = 0.2)\n",
    "\n",
    "validation = ImageDataGenerator(rescale=1/255.0, validation_split=0.2)\n",
    "\n",
    "test = ImageDataGenerator(rescale = 1/255.0)\n",
    "\n",
    "# create training set\n",
    "print('Training set:')\n",
    "train_generator = train.flow_from_directory(\"images/training\",\n",
    "                                            target_size = (image_height, image_width),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'binary',\n",
    "                                            classes = ['notflip', 'flip'],\n",
    "                                            subset = 'training',\n",
    "                                            seed = 11)\n",
    "\n",
    "# create validation set\n",
    "print('\\nValidation set:')\n",
    "validation_generator = validation.flow_from_directory(\"images/training\", # same directory as training data\n",
    "                                        target_size = (image_height, image_width),\n",
    "                                        batch_size = batch_size,\n",
    "                                        class_mode = 'binary',\n",
    "                                        classes = ['notflip', 'flip'],\n",
    "                                        subset = 'validation', # set as validation data\n",
    "                                        seed = 12,\n",
    "                                        shuffle=False)\n",
    "\n",
    "# create testing set  \n",
    "print('\\nTest set:')\n",
    "test_generator = test.flow_from_directory(\"images/testing\",\n",
    "                                          target_size = (image_height, image_width),\n",
    "                                          batch_size = batch_size,\n",
    "                                          class_mode = 'binary',\n",
    "                                          classes = ['notflip', 'flip'],\n",
    "                                          seed = 13,\n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "497919bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'notflip': 0, 'flip': 1}\n",
      "{'notflip': 0, 'flip': 1}\n",
      "{'notflip': 0, 'flip': 1}\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(train_generator.class_indices)\n",
    "print(validation_generator.class_indices)\n",
    "print(test_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3409e",
   "metadata": {},
   "source": [
    "# Call VGG16 <a class=\"anchor\" id=\"vgg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24380948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 13s 0us/step\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 25089     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14739777 (56.23 MB)\n",
      "Trainable params: 25089 (98.00 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# call the pretrained model VGG16 since it has been proved to \n",
    "# present the best accuracy compared to the VGG19 and Resnet50\n",
    "vgg = VGG16(input_shape=(224,224,3), \n",
    "            weights='imagenet', \n",
    "            include_top=False) # Leave out the last fully connected layer\n",
    "\n",
    "# Since we don’t have to train all the layers, we make them non_trainable\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Flatten the output layer to 1 dimension\n",
    "x = Flatten()(vgg.output)\n",
    "\n",
    "# Add a final sigmoid layer with 1 node for classification output\n",
    "prediction = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "\n",
    "# view the structure of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b58da845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "120/120 [==============================] - 49s 378ms/step - loss: 0.4581 - accuracy: 0.7973 - val_loss: 0.4679 - val_accuracy: 0.7301\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 51s 423ms/step - loss: 0.2160 - accuracy: 0.9284 - val_loss: 0.2075 - val_accuracy: 0.9017\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 57s 475ms/step - loss: 0.1606 - accuracy: 0.9556 - val_loss: 0.1327 - val_accuracy: 0.9498\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 56s 469ms/step - loss: 0.1166 - accuracy: 0.9660 - val_loss: 0.2156 - val_accuracy: 0.8933\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 59s 489ms/step - loss: 0.1004 - accuracy: 0.9739 - val_loss: 0.0968 - val_accuracy: 0.9603\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 60s 497ms/step - loss: 0.0891 - accuracy: 0.9744 - val_loss: 0.1057 - val_accuracy: 0.9456\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 62s 515ms/step - loss: 0.0828 - accuracy: 0.9765 - val_loss: 0.1746 - val_accuracy: 0.9121\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 57s 478ms/step - loss: 0.0746 - accuracy: 0.9791 - val_loss: 0.2892 - val_accuracy: 0.8724\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 54s 453ms/step - loss: 0.0670 - accuracy: 0.9807 - val_loss: 0.0759 - val_accuracy: 0.9686\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 55s 454ms/step - loss: 0.0540 - accuracy: 0.9875 - val_loss: 0.1401 - val_accuracy: 0.9247\n"
     ]
    }
   ],
   "source": [
    "# tell the model what cost and optimization method to use\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit(train_generator,\n",
    "              validation_data = validation_generator,\n",
    "              epochs=10,\n",
    "              batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ddb56",
   "metadata": {},
   "source": [
    "# Model evaluation <a class=\"anchor\" id=\"eva\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe42fb18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 15s 391ms/step - loss: 0.0619 - accuracy: 0.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06192483752965927, 0.9765493869781494]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4376813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 10s 255ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       307\n",
      "           1       1.00      0.96      0.98       290\n",
      "\n",
      "    accuracy                           0.98       597\n",
      "   macro avg       0.98      0.98      0.98       597\n",
      "weighted avg       0.98      0.98      0.98       597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# see how well each class is predicted\n",
    "preds=model.predict(test_generator)\n",
    "y_test = test_generator.classes\n",
    "y_pred = np.where(preds > 0.5, 1, 0)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63d3e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[306   1]\n",
      " [ 13 277]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe037ab",
   "metadata": {},
   "source": [
    "# Conclusion <a class=\"anchor\" id=\"con\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f79327",
   "metadata": {},
   "source": [
    "We can see that as compared to the previous regular CNN, the performance of VGG16 is utterly outstanding when only one image is incorrectly classified despite using 10 epochs. And since the size of this dataset is pretty small, the power of transformer is not clearly noticeable when VGG16 has achieved **0.98** of accuracy score and a regular CNN gains **0.95**. However, VGG16 is efficient in terms of running time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
